{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a29787d",
   "metadata": {},
   "source": [
    "# FIRSTRank\n",
    "\n",
    "We now demonstrate how to use `FIRSTRank` for factor importance ranking. If you have not installed `pyfirst`, please uncomment and run `%pip install pyfirst` below before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577ff6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyfirst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83423b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599de9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyfirst import FIRSTRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d82f46",
   "metadata": {},
   "source": [
    "## Simulate Data\n",
    "\n",
    "We simulate clean data from the Ishigami function \n",
    "\n",
    "$$\n",
    "    y = f(X) = \\sin(X_{1}) + 7\\sin^2(X_{2}) + 0.1X_{3}^{4}\\sin(X_{1}),\n",
    "$$\n",
    "\n",
    "where the input $X$ are independent features uniformly distributed on $[-\\pi,\\pi]^{3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46aa9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ishigami(x):\n",
    "    x = -np.pi + 2 * np.pi * x\n",
    "    y = np.sin(x[0]) + 7 * np.sin(x[1])**2 + 0.1 * x[2]**4 * np.sin(x[0])\n",
    "    return y\n",
    "\n",
    "np.random.seed(43)\n",
    "n = 10000\n",
    "p = 3\n",
    "X = np.random.uniform(size=(n,p))\n",
    "y = np.apply_along_axis(ishigami, 1, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e5762",
   "metadata": {},
   "source": [
    "## Run FIRSTRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506eff80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ranking': array([1, 0, 2]),\n",
       " 'explained_variance': array([0.44777553, 0.75116322, 1.        ])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIRSTRank(X, y, noise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010a81",
   "metadata": {},
   "source": [
    "This shows that the ranking of importance is by factor 1, factor 0, and factor 2. Factor 1 can explain 45% of the model variance, Factor 1 and 2 together can explain 75% of the model variance, and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31dd71a",
   "metadata": {},
   "source": [
    "## Noisy Data\n",
    "\n",
    "We now look at the estimation performance on the noisy data $y = f(X) + \\epsilon$ where $\\epsilon\\sim\\mathcal{N}(0,1)$ is the random error. For noisy data, `FIRSTRank` implements the Noise-Adjusted Nearest-Neighbor estimator in Huang and Joseph (2025), which corrects the bias by the Nearest-Neighbor estimator from Broto et al. (2020) when applied on noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef9eaac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ranking': array([1, 0, 2, 3, 4]),\n",
       " 'explained_variance': array([0.36974645, 0.67381678, 1.        , 1.        , 1.        ])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(43)\n",
    "n = 10000\n",
    "p = 3\n",
    "X = np.random.uniform(size=(n,p))\n",
    "y = np.apply_along_axis(ishigami, 1, X) + np.random.normal(size=n)\n",
    "X = np.hstack([X, np.zeros((n, 2))])\n",
    "\n",
    "FIRSTRank(X, y, noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3219e1",
   "metadata": {},
   "source": [
    "Factor 3 and 4 are non-important variables, so we would expect Factor 0, 1, 2 can explain all the model variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e36d9",
   "metadata": {},
   "source": [
    "For more details about `FIRSTRank`, please Huang and Joseph (2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef56e3",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Huang, C., & Joseph, V. R. (2025). Factor Importance Ranking and Selection using Total Indices. Technometrics.\n",
    "\n",
    "Sobol', I. M. (2001). Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates. Mathematics and computers in simulation, 55(1-3), 271-280.\n",
    "    \n",
    "Broto, B., Bachoc, F., & Depecker, M. (2020). Variance reduction for estimation of Shapley effects and adaptation to unknown input distribution. SIAM/ASA Journal on Uncertainty Quantification, 8(2), 693-716.\n",
    "\n",
    "Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazaré, P.E., Lomeli, M., Hosseini, L., & Jégou, H., (2024). The Faiss library. arXiv preprint arXiv:2401.08281.\n",
    "    \n",
    "Vakayil, A., & Joseph, V. R. (2022). Data twinning. Statistical Analysis and Data Mining: The ASA Data Science Journal, 15(5), 598-610."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
